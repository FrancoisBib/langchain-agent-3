# LangChain Chatbot

## Overview

**LangChain‑Chatbot** is a modular, extensible chatbot built on top of the **LangChain** framework. It demonstrates how to combine Large Language Models (LLMs) with Retrieval‑Augmented Generation (RAG) techniques to create context‑aware, knowledge‑driven conversational agents.

The repository provides:
- A **minimal yet production‑ready** example of a LangChain RAG pipeline.
- Clear entry points for **custom LLMs**, **vector stores**, and **document loaders**.
- A **FastAPI** (or Flask) based HTTP interface for easy integration with front‑ends.
- Comprehensive documentation and tests to help developers get started quickly and contribute effectively.

---

## Table of Contents

- [Features](#features)
- [Architecture Diagram](#architecture-diagram)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Running the Bot](#running-the-bot)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

---

## Features

- **RAG (Retrieval‑Augmented Generation)**: Combine a vector store with an LLM to answer questions using external knowledge bases.
- **Pluggable Components**: Swap out the LLM, embeddings model, vector store, or document loader with a single line change.
- **Async FastAPI Server**: Ready‑to‑use HTTP API (`/chat`) that streams responses.
- **Docker Support**: Official Dockerfile for reproducible environments.
- **Extensive Tests**: Unit and integration tests using `pytest` and `httpx`.
- **Typed Settings**: Pydantic‑based configuration for type‑safe environment variables.

---

## Architecture Diagram

```
+----------------+        +-------------------+        +-------------------+
|   Document     |  -->   |   Embedding Model |  -->   |   Vector Store    |
|   Loader(s)    |        |   (OpenAI, HF…)   |        |   (FAISS, Chroma) |
+----------------+        +-------------------+        +-------------------+
        |                                          |
        |                                          v
        |                                 +-------------------+
        |                                 |   Retriever       |
        |                                 +-------------------+
        |                                          |
        |                                          v
        |                                 +-------------------+
        |                                 |   LLM (ChatGPT,   |
        |                                 |   Llama, etc.)    |
        |                                 +-------------------+
        |                                          |
        +-------------------   RAG   -------------------+
                               Pipeline
```

---

## Installation

### Prerequisites

- Python **3.10** or newer
- **Poetry** (recommended) or `pip`
- Access to an LLM provider (e.g., OpenAI API key) **or** a locally hosted model
- (Optional) Docker if you prefer containerised execution

### Using Poetry (recommended)

```bash
# Clone the repository
git clone https://github.com/your-org/langchain-chatbot.git
cd langchain-chatbot

# Install dependencies
poetry install

# Activate the virtual environment
poetry shell
```

### Using pip

```bash
pip install -r requirements.txt
```

### Environment Variables

Create a `.env` file at the project root (or export variables directly). Example:

```dotenv
# LLM provider configuration
OPENAI_API_KEY=sk-****************

# Embedding model (optional – defaults to OpenAI embeddings)
EMBEDDING_MODEL=text-embedding-ada-002

# Vector store configuration (FAISS is default, Chroma can be used via CHROMA_*)
FAISS_INDEX_PATH=./data/faiss_index

# FastAPI settings
HOST=0.0.0.0
PORT=8000
```

---

## Quick Start

### 1️⃣ Index your knowledge base

```bash
python -m src.indexer --source ./data/documents
```

The command loads all supported document types (`.txt`, `.pdf`, `.md`, etc.), creates embeddings, and stores them in the configured vector store.

### 2️⃣ Run the API server

```bash
uvicorn src.main:app --host $HOST --port $PORT
```

The server will expose a single endpoint:

- **POST** `/chat` – body `{ "question": "Your query" }`
- Returns a streaming JSON response with the generated answer.

### 3️⃣ Test with `curl`

```bash
curl -X POST http://localhost:8000/chat \
     -H "Content-Type: application/json" \
     -d '{"question": "What is Retrieval‑Augmented Generation?"}'
```

You should receive a concise, source‑cited answer generated by the LLM using the retrieved documents.

---

## Configuration

All runtime options are defined in `src/config.py` using **Pydantic Settings**. The most common knobs:

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_MODEL` | Identifier of the LLM to use (e.g., `gpt-3.5-turbo`). | `gpt-3.5-turbo` |
| `EMBEDDING_MODEL` | Embedding model name. | `text-embedding-ada-002` |
| `VECTOR_STORE` | `faiss` or `chroma`. | `faiss` |
| `TOP_K` | Number of retrieved documents per query. | `4` |
| `MAX_TOKENS` | Max tokens for LLM generation. | `512` |
| `TEMPERATURE` | Sampling temperature. | `0.0` |

You can override any setting via environment variables or by passing a custom `Settings` object when creating the `Chatbot` class.

---

## Running the Bot

### As a Docker Container

```bash
# Build the image
docker build -t langchain-chatbot .

# Run the container (make sure to pass your .env file)
docker run -d \
  --name lc-chatbot \
  -p 8000:8000 \
  --env-file .env \
  langchain-chatbot
```

### Development Mode (Hot‑Reload)

```bash
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

---

## Testing

The project uses **pytest**. To run the full suite:

```bash
pytest -vv
```

Unit tests cover:
- Document loading & chunking
- Embedding generation (mocked)
- Retrieval logic
- FastAPI endpoint behaviour

Integration tests spin up an in‑memory FAISS index and a mock LLM to ensure the end‑to‑end RAG flow works as expected.

---

## Contributing

We welcome contributions! Follow these steps:

1. **Fork** the repository and clone your fork.
2. Create a **feature branch** (`git checkout -b feat/your-feature`).
3. Install dependencies with Poetry (`poetry install`).
4. Write tests for new functionality.
5. Ensure the test suite passes (`pytest`).
6. Open a **Pull Request** with a clear description of the change.

### Code Style

- Run **black** and **ruff** before committing:
  ```bash
  poetry run black .
  poetry run ruff check . --fix
  ```
- Follow the existing project structure (`src/` for source code, `tests/` for tests).

### Documentation

If you add new public classes or functions, update the corresponding docstrings and, when appropriate, extend the README or the `docs/` folder.

---

## License

Distributed under the **MIT License**. See `LICENSE` for more information.

---

## Acknowledgements

- **LangChain** – the core framework that powers the RAG pipeline.
- **OpenAI**, **Hugging Face**, **FAISS**, **Chroma** – for the underlying models and vector stores.
- Community contributors who help keep the project up‑to‑date.
