# LangChain Chatbot

## ğŸ“– Overview

**LangChainâ€‘Chatbot** is a minimal yet productionâ€‘ready reference implementation of a Retrievalâ€‘Augmented Generation (RAG) chatbot built on top of **[LangChain](https://python.langchain.com/)**.  It demonstrates how to combine:

- **Large Language Models (LLMs)** for naturalâ€‘language generation
- **Vector stores** for semantic retrieval of documents
- **Chains & agents** to orchestrate the retrievalâ€‘generation workflow
- **FastAPI** (or optionally Streamlit) as a thin HTTP layer for integration with UI frontâ€‘ends or other services

The repository is deliberately kept simple so that developers can:

1. **Learn** the core concepts of LangChain and RAG.
2. **Bootstrap** their own chatbot projects with a solid foundation.
3. **Contribute** improvements, new integrations, or tests.

---

## ğŸ› ï¸ Features

- ğŸ“š **Document ingestion** â€“ Supports PDF, TXT, Markdown, and CSV files.
- ğŸ” **Semantic search** â€“ Uses OpenAI embeddings (or any HuggingFace embedding model) stored in a ChromaDB vector store.
- ğŸ¤– **RAG pipeline** â€“ Retrieves the most relevant chunks and passes them to an LLM (OpenAI `gptâ€‘4o`, Anthropic `claudeâ€‘3`, etc.) with a concise system prompt.
- âš¡ **Streaming responses** â€“ Optional tokenâ€‘wise streaming for a more interactive UI experience.
- ğŸ§© **Modular architecture** â€“ All components (loader, splitter, embedder, vector store, LLM) are interchangeable via LangChain interfaces.
- ğŸ§ª **Testing utilities** â€“ Pytest fixtures for unitâ€‘testing the retrieval and generation steps.
- ğŸ“¦ **Docker support** â€“ Multiâ€‘stage Dockerfile for reproducible local development and production deployment.

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Version |
|-------------|---------|
| Python      | `>=3.9` |
| Docker      | `>=20.10` |
| OpenAI API key (or alternative LLM provider) |

### 1. Clone the repository

```bash
git clone https://github.com/yourâ€‘org/langchain-chatbot.git
cd langchain-chatbot
```

### 2. Install dependencies (virtualâ€‘env recommended)

```bash
python -m venv .venv
source .venv/bin/activate   # on Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Set environment variables

Create a `.env` file at the project root (or export variables in your shell):

```dotenv
# LLM provider â€“ currently supported: openai, anthropic, together
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-*****
# Optional â€“ change the model
OPENAI_MODEL=gpt-4o-mini
# Vector store configuration (defaults to local ChromaDB)
CHROMA_PERSIST_DIR=./chroma_db
```

### 4. Ingest your knowledge base

Place any documents you want the bot to know about inside the `data/` directory, then run:

```bash
python scripts/ingest.py
```

The script will:
1. Load files using LangChain loaders.
2. Split them into chunks (default: 1â€¯000 tokens, 200 token overlap).
3. Compute embeddings and persist them in the Chroma vector store.

### 5. Run the API server

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The chatbot endpoint is now available at `POST /chat`:

```json
# Request payload
{
  "message": "Explain the difference between supervised and unsupervised learning."
}
```

The response contains the generated answer and the source documents used for retrieval.

---

## ğŸ“š Detailed Usage

### 5.1 API contract

| Method | Endpoint | Body | Returns |
|--------|----------|------|---------|
| `POST` | `/chat` | `{ "message": "string" }` | `{ "answer": "string", "sources": [{"page_content": "...", "metadata": {...}}] }` |
| `GET`  | `/health` | â€“ | `{ "status": "ok" }` |

### 5.2 Customising the Retrieval Chain

The core RAG logic lives in `app/rag.py`.  To swap components, modify the factory functions:

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA

# 1ï¸âƒ£ Embeddings â€“ replace with HuggingFaceEmbeddings if you prefer an openâ€‘source model
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

# 2ï¸âƒ£ Vector store â€“ any LangChainâ€‘compatible store works (FAISS, Pinecone, Weaviate, â€¦)
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

# 3ï¸âƒ£ LLM â€“ change model or provider
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.0)

# 4ï¸âƒ£ RetrievalQA chain â€“ you can tweak the prompt template here
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
)
```

### 5.3 Streaming responses (optional)

If you enable streaming in the `.env` (`STREAMING=true`), the FastAPI endpoint will use Serverâ€‘Sent Events (SSE).  Frontâ€‘ends can consume the stream tokenâ€‘byâ€‘token for a chatâ€‘like experience.

---

## ğŸ§ª Testing

```bash
pytest -q
```

The test suite covers:
- Document loading & splitting
- Vector store persistence & similarity search
- Endâ€‘toâ€‘end RAG generation (mocked LLM calls)

Add new tests under `tests/` to protect future contributions.

---

## ğŸ³ Docker

### Build the image

```bash
docker build -t langchain-chatbot:latest .
```

### Run the container (environment variables can be passed with `-e`)

```bash
docker run -p 8000:8000 \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/chroma_db:/app/chroma_db \
  -e OPENAI_API_KEY=sk-***** \
  langchain-chatbot:latest
```

The container starts the FastAPI server automatically.

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository.
2. **Create a branch** for your feature or bugâ€‘fix (`git checkout -b feat/awesomeâ€‘feature`).
3. **Write tests** for any new functionality.
4. **Run the full test suite** (`pytest`).
5. **Submit a Pull Request** with a clear description of the change.

### Code style
- Use **Black** for formatting (`black .`).
- Type hints are mandatory (`mypy` passes).
- Lint with **ruff** (`ruff check .`).

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€“ see the `LICENSE` file for details.

---

## ğŸ“š Further Reading

- LangChain Documentation â€“ <https://python.langchain.com/docs/>
- Retrievalâ€‘Augmented Generation â€“ <https://arxiv.org/abs/2005.11401>
- OpenAI Embeddings â€“ <https://platform.openai.com/docs/guides/embeddings>

---

*Happy coding!*